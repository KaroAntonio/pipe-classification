hours: 0

:: DATABASE MERGING ::
for Break records
if breaks record break rate > 5, it was replaced, else repaired
if not in break record assume pipe was fine
if age of the pipe < 40/50 yrs old it's probably in good shape, considered 'new'
	* the oldest pipes are 36 years old

W/O date is repair date	
num recent breaks, is repair in last 5 years, (before 2010)
remaaining service life is based on formula from condition model
mi = remaining servi
pipe type: in transmission
hydr capacity: 
env sensitive: count of close landmarks in trans data
accessibility: esa (easement col), calc, if there are any crossings, then easement is 1, else 0
conformance: calc from chart
water quality: chart

TODO: optimize Score mult (eg. x8)

----------------------------------------------------------------
Condition Score
RSL: remaining service life score
MI: maintenance index
* generate total number of breaks
* generate recent number of breaks < total num breaks
BRKS_FVYRS_SCR

PCSDesc: Expert Opinion
AR: Expert Opinion

Score = SUM () * 8

OUT: 2 cols
score (num) / level (low/med/hi)
----------------------------------------------------------------
Criticality Score
CRITICAL_SCR: expert opinion
(DIAMETER SCR+ ENVMT SCR+ ACCESS SCR)*6

OUT: 2 cols
score (num) / level (very-lo/mod-lo/med/mod-hi/hi)
----------------------------------------------------------------
Performance Score
QUALITY_SCR
STANDARD_CONFOR
HEAD_LOSS_SCR: hydraulic capacity
DIAMETER: 
^^^ Those three summed and mult by 10	

OUT: 2 cols
score (num) / level (very-lo/mod-lo/med/mod-hi/hi)
----------------------------------------------------------------
^^^ compare w other (weka/scipy) models

SEVERITY: expert opinion

** We are trying to do better than expert opinion

Input variables:
DIAMETER, LAND_USE,

FINAL MODEL OUTPUT
TOTAL PAN Score - High Score correlates to High Mitigation Class
Mitigation - 1:Do Nothing, 2:Repair, 3:Replace, 4: Upsize

Criticality Model : investigate the use of other models here

FOR MODEL OUTPUT 
scale model output to be in same range as the expert opinion
have a column for calculated score for each model
add normalized column for each of  the  above
try implement model to predict

TODO :: dec 16
	convert NaiveBayes to 2.7
	refactor functions into reasonable modules


----------------------------------------------------------------

TODO :: feb 14, 2017
	convert NaiveBayes to 2.7
	refactor functions into reasonable modules
	add coll id to output files
		id col: FID

	add global run function that you can call from ARC GIS

----------------------------------------------------------------

TODO :: march 4, 2017
	1. generate mitigation column based on rules	
	2. generate mitigation output with naivebayes
	3. look into bernoulli, gaussian for naive bayes

MITIGATION MATRIX
	for Mitigation		For Condition, Criticality and Performance
	1= Do Nothing		1= Very Low
	2= Upsize			2= Moderately Low
	3=Rehab				3= Medium
	4=Replace			4= Moderately High
	5= Very High
 
----------------------------------------------------------------

TODO :: march 18, 2017

TASKS:
	1. Performance Target, Criticitality Target, Condition Target
	2. Make fake target for the second layer
	3. bernoulli/gaussian naive bayes
	4. Integrate mitigation layer
	

HOURS:
	1. One Hour Outstanding. 

----------------------------------------------------------------

TODO :: may 16, 2017

Get weights two layer naive bayes
Get weights one layer naive bayes
train on expert opinion and final output

train first layer using final output as the target
train second layer using final output as target
		outputs of first layer
		

What does the weights mean? 	


----------------------------------------------------------------

DEV :: June 5, 2017

for the cond, crit, perf models i used TARGET cols, is this what i want?

hours: 4 more


----------------------------------------------------------------

DEV :: Dec 10, 2017

hours 2 more (at 11:40)

----------------------------------------------------------------

DEV :: June 8, 2019

restructured codebase to keep only top level scripts in root dir
Are run_var_weight_assessment.py and run_determine_best_vars.py doing the same thing??
no they are not! (1) determines the weight (relative importance) of vars, while determine best vars determines which vars to use. 

hours 1

----------------------------------------------------------------

DEV :: June 9, 2019

starting @ 11:19

trying to understand what a probability density function of this is wrt to the model. 
ok there is what a pdf is but what i understand ramona to be asking is perhaps a little different:
So it seems that she's asking: what is the probability that the model predicts output 1 given inputs: A, B, C
this will always be a 100% probability for one of the labels and 0% for the rest

but likely she's asking what is the probability that the model has learned that it then uses to chose the output label


